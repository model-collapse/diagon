# P0.1 SIMD BM25 Scoring - Analysis and Next Steps

**Date**: 2026-01-30
**Status**: First attempt - Performance regression identified
**Current Result**: 9% slower (121 μs vs 111 μs)

## What Happened

### Implementation Approach
Created `TermScorerSIMD` that:
1. Buffers 8 documents at a time
2. Collects their frequencies and norms
3. Scores all 8 with one AVX2 instruction
4. Returns buffered scores one at a time

### Profiling Results

**Before (Scalar):**
```
28.78%  TermScorer::score()              [BM25 scoring - SLOW]
21.32%  TopScoreDocCollector::collect()
 7.00%  SimplePostingsEnum::freq()
 6.89%  SimplePostingsEnum::nextDoc()
```

**After (SIMD with batching):**
```
32.79%  TermScorerSIMD::nextDoc()        [NEW BOTTLENECK!]
16.40%  TopScoreDocCollector::collect()
 8.14%  Lucene104NormsReader::longValue()
 8.03%  SimplePostingsEnum::nextDoc()
 3.91%  TermScorerSIMD::score()          [Scoring IS faster! 7x improvement]
```

### Analysis

**GOOD NEWS**: Scoring IS 7x faster (28.78% → 3.91%) ✅
**BAD NEWS**: Batching overhead dominates (nextDoc: 0% → 32.79%) ❌

**Root Cause**:
`fillBuffer()` calls these 8 times per batch:
- `postings_->nextDoc()`
- `norms_->advanceExact(doc)`
- `norms_->longValue()`

This overhead (8 function calls with virtual dispatch) exceeds the SIMD benefit.

### Performance Impact

- **Scoring improvement**: 28.78% → 3.91% = 24.87% savings ✅
- **Batching cost**: 0% → 32.79% = 32.79% added ❌
- **Net result**: -7.92% (slower overall)

## Why Batching Failed

1. **Virtual function overhead**: `postings_->nextDoc()` is virtual (v-table lookup × 8)
2. **Norms access overhead**: Two function calls per document (advanceExact + longValue)
3. **Memory access pattern**: Accessing norms for 8 documents may cause cache misses
4. **Branch overhead**: fillBuffer() has multiple branches

## Alternative Approaches

### Option 1: Collector-Level Batching (Best)

Instead of batching in Scorer, batch in TopScoreDocCollector:

```cpp
class TopScoreDocCollectorSIMD {
private:
    static constexpr int BATCH_SIZE = 8;

    struct DocumentBatch {
        int docs[BATCH_SIZE];
        float scores[BATCH_SIZE];
        int size;
    };

    DocumentBatch batch_;

public:
    void collect(int doc) override {
        float score = scorer_->score();

        // Add to batch
        batch_.docs[batch_.size] = doc;
        batch_.scores[batch_.size] = score;
        batch_.size++;

        // Process batch when full
        if (batch_.size == BATCH_SIZE) {
            processBatchSIMD();
            batch_.size = 0;
        }
    }

private:
    void processBatchSIMD() {
        // Load 8 scores
        __m256 scores_vec = _mm256_loadu_ps(batch_.scores);
        __m256 min_score_vec = _mm256_set1_ps(minScore_);

        // Compare: which scores beat minimum?
        __m256 mask = _mm256_cmp_ps(scores_vec, min_score_vec, _CMP_GT_OQ);

        // Process matches
        int mask_int = _mm256_movemask_ps(mask);
        for (int i = 0; i < 8; i++) {
            if (mask_int & (1 << i)) {
                insertIntoHeap(batch_.docs[i], batch_.scores[i]);
            }
        }
    }
};
```

**Advantages**:
- No extra postings/norms access
- Scores already computed (no overhead)
- Can use SIMD for heap operations
- Lower risk (isolated to collector)

**Expected Impact**: +15-25% search speed

### Option 2: SIMD Single-Document Scoring

Skip batching entirely, use SIMD for individual score computation:

```cpp
float TermScorer::score() const {
    long norm = norms_->longValue();
    int freq = freq_;

#ifdef DIAGON_HAVE_AVX2
    // Use SIMD for single score (still faster than scalar)
    __m256 freq_vec = _mm256_set1_ps(freq);
    __m256 norm_vec = decodeNormSIMD(norm);
    // ... compute with SIMD
    float result[8];
    _mm256_storeu_ps(result, score_vec);
    return result[0];
#else
    // Scalar fallback
    return simScorer_.score(freq, norm);
#endif
}
```

**Advantages**:
- No batching overhead
- Simple implementation
- Still faster than scalar (SIMD ops are fast even for 1 value)

**Disadvantages**:
- Not using full SIMD width (wasting 7/8 of vector)
- May not be much faster (estimate: 10-15% vs 40-50%)

### Option 3: Hybrid Approach

Buffer postings decoding (already done efficiently), score in batches:

```cpp
// In SimplePostingsEnum, decode 128 docIDs with StreamVByte
// Then in Scorer, score them in batches of 8
```

**Advantage**: Amortize postings decoding cost
**Disadvantage**: More complex, requires postings format changes

## Recommendation

**Implement Option 1: Collector-Level Batching**

**Rationale**:
1. Lowest overhead (no extra postings/norms access)
2. Can optimize both scoring AND heap operations
3. Isolated change (only TopScoreDocCollector)
4. Expected impact: +15-25% search speed

**Implementation Plan**:
1. Add batch buffers to TopScoreDocCollector
2. Buffer 8 (doc, score) pairs before processing
3. Use SIMD to compare all 8 scores with minScore
4. Use SIMD for heap updates if beneficial
5. Handle partial batches (<8) at end

**Estimated Effort**: 1-2 days
**Risk**: Low (collector isolated from rest of system)

## Lessons Learned

1. **Batching has overhead**: Virtual function calls, memory access patterns
2. **Profile early**: Caught regression before shipping
3. **SIMD benefit vs overhead**: Must ensure SIMD savings > batching cost
4. **Alternative approaches**: Multiple ways to apply SIMD, choose wisely

## Next Steps

1. **Revert P0.1** - Remove TermScorerSIMD batching
2. **Implement Option 1** - Collector-level batching
3. **Benchmark** - Measure actual improvement
4. **Profile** - Verify bottleneck reduced
5. **If successful** - Document and move to P0.2

## Conclusion

First SIMD attempt shows:
- ✅ SIMD scoring works (7x faster)
- ❌ Batching overhead too high at Scorer level
- ✅ Alternative approaches identified
- ✅ Learned valuable lessons about batching costs

**Status**: P0.1 in progress, trying Option 1 next

---

## Option 1 Implementation Results

**Date**: 2026-01-30
**Implementation**: Collector-Level Batching in TopScoreDocCollector

### What Was Implemented

Modified `TopScoreDocCollector` to batch documents before processing:

1. **Buffering**: Added 8-element buffers for docs and scores in `TopScoreLeafCollector`
2. **Batch Processing**: `collect()` adds to batch, flushes when full
3. **SIMD Filtering**: `flushBatch()` uses AVX2 to compare all 8 scores with minScore
4. **Bitmask Selection**: Only processes documents that beat minScore

**Key Code**:
```cpp
void flushBatch() {
    if (static_cast<int>(parent_->pq_.size()) < parent_->numHits_) {
        // Queue not full yet, add all documents
        for (int i = 0; i < batchPos_; i++) {
            collectSingle(docBatch_[i], scoreBatch_[i]);
        }
    } else {
        // Queue full, use SIMD to filter
        float minScore = parent_->pq_.top().score;
        __m256 minScore_vec = _mm256_set1_ps(minScore);
        __m256 scores_vec = _mm256_loadu_ps(scoreBatch_);
        __m256 mask = _mm256_cmp_ps(scores_vec, minScore_vec, _CMP_GT_OQ);
        int mask_int = _mm256_movemask_ps(mask);

        // Process only matches
        for (int i = 0; i < batchPos_; i++) {
            if (mask_int & (1 << i)) {
                collectSingle(docBatch_[i], scoreBatch_[i]);
            }
        }
    }
    batchPos_ = 0;
}
```

### Performance Results

**Benchmark**: BM_TermQuerySearch/10000 (10,000 documents, 5 repetitions)

| Implementation | Latency | vs Baseline | Status |
|----------------|---------|-------------|--------|
| Baseline (scalar) | 111 μs | - | Reference |
| Scorer batching | 121 μs | -9% (slower) | ❌ Failed |
| **Collector batching** | **105 μs** | **+5.4%** | ✅ Success |

**Median**: 105 μs
**Stddev**: 0.225 μs (very stable)

### Analysis: Why 5.4% Instead of 15-25%?

The improvement is positive but lower than projected. Reasons:

1. **SIMD benefit only when queue is full**: The SIMD filtering helps only after we've collected `numHits` documents. Before that, we just add batching overhead with no benefit.

2. **Still processing individually**: Even with SIMD filtering, we still call `collectSingle()` for each matching document, which involves:
   - Priority queue operations (push/pop)
   - Score comparisons
   - Doc ID comparisons

3. **Limited by heap operations**: The priority queue operations (push/pop/top) are now a larger portion of the remaining cost.

4. **No scorer optimization**: We're still calling `scorer_->score()` for every document (scalar BM25). The original 28.78% bottleneck is still there.

### What Worked

✅ **Avoided batching overhead**: Unlike scorer-level batching, no extra postings/norms access
✅ **Stable improvement**: Consistent 5-6 μs reduction across all runs
✅ **No regressions**: Code remains clean and maintainable
✅ **Low risk**: Isolated change in collector only

### What Didn't Work

❌ **BM25 scoring still scalar**: The original 28.78% bottleneck remains untouched
❌ **Heap operations unchanged**: Priority queue still processes one-at-a-time
❌ **Limited SIMD utilization**: Only used for score comparison, not computation

### Root Cause of Gap

The **real bottleneck** is still **BM25 scoring** (28.78% CPU). Collector-level batching only optimizes the filtering step (comparing scores with minScore), which is a small part of the overall work.

**The actual problem**: We batch AFTER scoring, so we never reduced the scoring cost.

### Revised Understanding

**Bottleneck hierarchy**:
1. **28.78% - BM25 scoring** ← Still scalar, unchanged
2. **21.32% - Collector operations** ← 5.4% reduction achieved
3. **13.89% - Postings traversal** ← Unchanged

**To get 15-25% improvement**, we need to **optimize the scoring itself**, not just the collection.

### Next Steps

**Option A: Combine Both Approaches** (Recommended)
1. Keep collector batching (5.4% gain)
2. Add SIMD BM25 scoring (without batching overhead)
3. Expected: 5.4% + (28.78% × 0.5 reduction) = **20% total**

**Option B: SIMD in SimScorer**
- Implement `BM25ScorerSIMD::score()` for single document
- Use SIMD instructions but process one doc at a time
- Expected: +10-15% from scoring speedup
- Combined with collector batching: 15-20% total

**Option C: Postings-Level Batching** (Higher risk)
- Modify PostingsEnum to decode 128 docs at once (StreamVByte)
- Batch-score them with SIMD
- Higher complexity, requires format changes

### Recommendation

**Implement Option A**: Add SIMD to the existing `SimScorer::score()` method.

**Rationale**:
- Targets the actual bottleneck (28.78% scoring)
- Keeps collector batching benefits
- Low risk (just vectorize the math)
- Expected total: 20-25% improvement

### Attempt 3: SIMD Single-Document Scoring (Failed)

**Date**: 2026-01-30
**Approach**: Add SIMD to `SimScorer::score()` for individual documents

**Implementation**:
- Added AVX2 instructions to score() method
- Broadcast single values to 256-bit vectors
- Process one document with vector instructions
- Extract first element from result vector

**Theory**: SIMD instructions might be faster even for one value due to optimized instruction execution.

**Result**: **❌ FAILED** - Made things worse!

| Implementation | Latency | Change |
|----------------|---------|--------|
| Collector batching only | 105 μs | Baseline |
| + SIMD single-doc | 116 μs | **-10.5% (slower!)** |

**Why It Failed**:
1. **Vector creation overhead**: `_mm256_set1_ps()` for each scalar value
2. **No parallelism benefit**: Processing one value, wasting 7/8 of vector width
3. **Memory alignment cost**: `_mm256_store_ps()` requires aligned memory
4. **More instructions overall**: SIMD path has more operations than simple scalar math

**Key Lesson**: SIMD requires **true parallelism** to be beneficial. Single-value processing is slower.

---

### Final Analysis: The Fundamental Problem

**The Bottleneck**: 28.78% CPU in BM25 scoring (`SimScorer::score()`)

**What We Tried**:
1. **Scorer-level batching** (Option 3): Batch 8 docs, score with SIMD → ❌ -9% (batching overhead 32.79%)
2. **Collector-level batching** (Option 1): Batch 8 docs, filter with SIMD → ✅ +5.4%
3. **SIMD single-doc scoring** (Option 2): SIMD for one doc → ❌ -10.5% (overhead without benefit)

**Why Collector Batching Helped (but Limited)**:
- ✅ No extra postings/norms access (scores already computed)
- ✅ SIMD comparison of 8 scores with minScore is genuinely parallel
- ❌ BUT: Only optimizes the **filtering step**, not the **scoring itself**
- ❌ The 28.78% scoring bottleneck remains unchanged

**Why True SIMD Batching Failed**:
- ❌ Scoring 8 docs requires collecting 8 (freq, norm) pairs
- ❌ This means calling `postings->nextDoc()` × 8 (virtual dispatch overhead)
- ❌ And `norms->longValue()` × 8 (more function calls)
- ❌ This overhead (32.79%) exceeds SIMD benefit

**The Architectural Dilemma**:
```
To get SIMD benefit, need:   But this requires:
  Score 8 docs at once    →   Collect 8 (freq, norm) pairs first
                          →   Call postings_->nextDoc() × 8
                          →   Call norms_->longValue() × 8
                          →   32.79% overhead > SIMD savings
```

**What Actually Works**:
- Collector batching: Batch AFTER scoring, use SIMD for filtering only
- Result: 5.4% improvement (optimizes 21.32% collector overhead)
- The 28.78% scoring bottleneck remains scalar

---

### Final Recommendation

**P0.1 Status**: ✅ **Complete with 5.4% improvement**

**Why Stop Here**:
1. Collector-level batching is the **practical maximum** for this architecture
2. True SIMD scoring requires architectural changes (too risky for P0)
3. Other optimizations (P0.2, P0.4) have better ROI

**Better Approach for Scoring**:
- Move to **P0.4: StreamVByte SIMD** (decode 128 docs at once)
- This amortizes postings access overhead across many documents
- Can then batch-score decoded documents with lower overhead
- Expected: +15-20% from postings optimization alone

**Alternative Long-Term Solution**:
- **Redesign postings iterator** to expose batch decoding API
- `postings->nextBatch(buffer, 128)` → Returns 128 (doc, freq) pairs
- Then score in chunks of 8 with SIMD
- Requires significant refactoring (P2 work)

### Final Status

**P0.1 Collector-Level Batching**: ✅ **Complete** (5.4% improvement, 111 μs → 105 μs)
**P0.1 SIMD BM25 Scoring**: ❌ **Not Achievable** (architectural constraints)

**Lessons Learned**:
1. ✅ Profiling caught regressions early (saved shipping bad code)
2. ✅ Tried multiple approaches systematically
3. ✅ Documented why each approach failed (valuable knowledge)
4. ❌ SIMD requires true parallelism - single-value SIMD is counterproductive
5. ❌ Batching overhead can exceed SIMD benefits
6. ✅ Architecture matters - collector batching works because it's post-scoring

**Next Steps**:
1. ✅ **Keep collector batching** (5.4% is real improvement)
2. Move to **P0.2: Custom Tokenizer** (+15-20% indexing)
3. Then **P0.4: StreamVByte SIMD** (+15-20% search)
4. Consider **P2: Batch Postings API** for true batch scoring

---

**Updated**: 2026-01-30
**Final**: 2026-01-30
**Generated**: 2026-01-30
**Author**: Claude Sonnet 4.5
