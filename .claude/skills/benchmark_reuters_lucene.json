{
  "name": "benchmark_reuters_lucene",
  "description": "Run Reuters-21578 dataset benchmark to compare Diagon with Lucene",
  "version": "1.0.0",
  "arguments": [
    {
      "name": "benchmark",
      "description": "Which benchmark to run: reuters (standard), wand (WAND optimization), or both (default: reuters)",
      "type": "string",
      "default": "reuters"
    },
    {
      "name": "build",
      "description": "Build the benchmark before running (default: true)",
      "type": "boolean",
      "default": true
    },
    {
      "name": "clean_index",
      "description": "Clean existing index before benchmarking (default: true)",
      "type": "boolean",
      "default": true
    },
    {
      "name": "save_results",
      "description": "Save results to timestamped file (default: true)",
      "type": "boolean",
      "default": true
    }
  ],
  "prompt": "You are executing the Reuters-21578 benchmark for the Diagon search engine.\n\n## Your Task\n\nRun Reuters-21578 dataset benchmark with these parameters:\n- Benchmark type: {{benchmark}}\n- Build first: {{build}}\n- Clean index: {{clean_index}}\n- Save results: {{save_results}}\n\n## About Reuters-21578 Dataset\n\nThe Reuters-21578 dataset is the standard IR benchmark used by Apache Lucene:\n- **Documents**: 21,578 news articles from 1987\n- **Location**: `/home/ubuntu/opensearch_warmroom/lucene/lucene/benchmark/work/reuters-out/`\n- **Content**: Financial news articles with titles, dates, and body text\n- **Use**: Enables direct comparison with Lucene's published benchmark results\n\n## Available Benchmarks\n\n### 1. ReutersBenchmark (Standard)\n**Executable**: `ReutersBenchmark`\n**Tests**:\n- Indexing performance (21,578 documents)\n- Single-term queries (dollar, oil, trade)\n- Boolean AND queries (oil AND price)\n- Boolean OR queries (2-term and 5-term)\n- Reports: throughput, latency, index size\n\n### 2. ReutersWANDBenchmark (WAND Optimization)\n**Executable**: `ReutersWANDBenchmark`\n**Tests**:\n- WAND (Weak AND) early termination\n- Block-max scoring optimization\n- Uses Google Benchmark framework\n- Detailed performance metrics\n\n### 3. Both (Run All)\nRuns both benchmarks sequentially for comprehensive results.\n\n## Execution Procedure\n\n### Step 1: Build Benchmarks (if build=true)\n\n```bash\ncd /home/ubuntu/diagon\n\n# Build the requested benchmark(s)\nif [[ \"{{benchmark}}\" == \"reuters\" || \"{{benchmark}}\" == \"both\" ]]; then\n    cd build\n    make ReutersBenchmark -j8\nfi\n\nif [[ \"{{benchmark}}\" == \"wand\" || \"{{benchmark}}\" == \"both\" ]]; then\n    cd build\n    make ReutersWANDBenchmark -j8\nfi\n```\n\nIf build directory doesn't exist, inform the user they need to run `/build_diagon target=benchmarks` first.\n\n### Step 2: Clean Index (if clean_index=true)\n\n```bash\n# Remove old index\nrm -rf /tmp/diagon_reuters_index\n```\n\nThis ensures a fresh benchmark without cached data.\n\n### Step 3: Verify Dataset Exists\n\n```bash\nls /home/ubuntu/opensearch_warmroom/lucene/lucene/benchmark/work/reuters-out/*.txt | head -5\n```\n\nIf dataset not found, report error and suggest downloading it.\n\n### Step 4: Run Benchmark(s)\n\n**For benchmark='reuters':**\n```bash\ncd /home/ubuntu/diagon/build/benchmarks\n./ReutersBenchmark\n```\n\n**For benchmark='wand':**\n```bash\ncd /home/ubuntu/diagon/build/benchmarks\n./ReutersWANDBenchmark\n```\n\n**For benchmark='both':**\n```bash\ncd /home/ubuntu/diagon/build/benchmarks\necho \"========================================\"\necho \"Running Standard Reuters Benchmark\"\necho \"========================================\"\n./ReutersBenchmark\n\necho \"\"\necho \"========================================\"\necho \"Running WAND Reuters Benchmark\"\necho \"========================================\"\n./ReutersWANDBenchmark\n```\n\n### Step 5: Save Results (if save_results=true)\n\n```bash\n# Create results directory\nmkdir -p /home/ubuntu/diagon/benchmark_results\n\n# Save with timestamp\nTIMESTAMP=$(date +%Y%m%d_%H%M%S)\nRESULTS_FILE=\"/home/ubuntu/diagon/benchmark_results/reuters_${TIMESTAMP}.txt\"\n\n# Run benchmark and save output\n./ReutersBenchmark | tee \"$RESULTS_FILE\"\n\necho \"\"\necho \"Results saved to: $RESULTS_FILE\"\n```\n\n### Step 6: Report Results\n\nProvide a summary including:\n\n**Indexing Performance:**\n- Documents indexed: 21,578\n- Indexing time (seconds)\n- Throughput (docs/sec)\n- Index size (MB)\n- Storage efficiency (bytes/doc)\n\n**Query Performance:**\n- Query types tested\n- P99 latency (microseconds) for each query\n- Number of hits for each query\n- Comparison notes (if available)\n\n**Key Metrics to Report:**\n1. Indexing throughput vs Lucene (target: similar or better)\n2. Query latency vs Lucene (target: 3-10x faster)\n3. Index size vs Lucene (target: competitive)\n4. Any performance anomalies or concerns\n\n**Critical Reminders from CLAUDE.md:**\n- \"Be Honest\": Report actual numbers, not predicted\n- \"Be Humble and Straight\": Don't bury issues in boilerplate\n- \"Insist Highest Standard\": Target 3-10x faster than Lucene\n- If we're slower, be ashamed and investigate why\n\n## Error Handling\n\n### Build Directory Missing\n```\n❌ Build directory not found!\nRun: /build_diagon target=benchmarks\n```\n\n### Benchmark Executable Missing\n```\n❌ Benchmark executable not found!\nRun: /build_diagon target=benchmarks\nThen: make ReutersBenchmark\n```\n\n### Dataset Missing\n```\n❌ Reuters dataset not found!\nExpected: /home/ubuntu/opensearch_warmroom/lucene/lucene/benchmark/work/reuters-out/\n\nTo download:\n1. cd /home/ubuntu/opensearch_warmroom/lucene/lucene/benchmark\n2. ant get-reuters\n3. Verify: ls work/reuters-out/*.txt | wc -l\n   Should show: 21578\n```\n\n### Benchmark Crashes\n- Check if ICU is linked: `ldd build/benchmarks/ReutersBenchmark | grep icu`\n- Try rebuilding: `/build_diagon target=benchmarks`\n- Check logs for specific errors\n\n## Performance Expectations\n\n### Indexing (21,578 documents)\n- **Lucene**: ~5,000-10,000 docs/sec\n- **Target**: Competitive (similar or better)\n- **Time**: Should complete in 2-5 seconds\n\n### Query Latency (P99)\n- **Single term**: <1ms\n- **Boolean AND**: <2ms\n- **Boolean OR (2-term)**: <3ms\n- **Boolean OR (5-term)**: <5ms\n\n### Index Size\n- **Expected**: 5-15 MB\n- **Per doc**: 250-700 bytes/doc\n- **Target**: Competitive with Lucene\n\n## Success Criteria\n\nA successful benchmark run must:\n- ✅ Index all 21,578 documents without errors\n- ✅ Complete all test queries successfully\n- ✅ Report reasonable performance numbers\n- ✅ Show no crashes or undefined behavior\n- ✅ Generate results for comparison with Lucene\n\n## Comparison Context\n\nRemind the user of Diagon's goals:\n- **Target**: 3-10x faster search than Apache Lucene\n- **Standard**: No excuse for falling behind\n- **Honesty**: Report actual numbers, annotate as \"experimented\"\n- **Improvement**: If slower, investigate and optimize\n\nNow execute the Reuters benchmark following these steps."
}
