{
  "name": "benchmark_diagon_multiterm",
  "description": "Benchmark Diagon multi-term query performance on Reuters-21578 dataset",
  "version": "1.0.0",
  "arguments": [
    {
      "name": "query_type",
      "description": "Query type to test: and (Boolean AND), or (Boolean OR), mixed (AND+OR), or all (default: all)",
      "type": "string",
      "default": "all"
    },
    {
      "name": "term_counts",
      "description": "Term counts to test: small (2-3 terms), medium (4-5 terms), large (6-10 terms), or all (default: all)",
      "type": "string",
      "default": "all"
    },
    {
      "name": "build",
      "description": "Build the benchmark before running (default: true)",
      "type": "boolean",
      "default": true
    },
    {
      "name": "iterations",
      "description": "Number of iterations per query for stable measurements (default: 100)",
      "type": "number",
      "default": 100
    },
    {
      "name": "save_results",
      "description": "Save results to timestamped file (default: true)",
      "type": "boolean",
      "default": true
    }
  ],
  "prompt": "You are executing a focused multi-term query benchmark for Diagon.\n\n## Your Task\n\nBenchmark Diagon's multi-term query performance with these parameters:\n- Query type: {{query_type}}\n- Term counts: {{term_counts}}\n- Build first: {{build}}\n- Iterations: {{iterations}}\n- Save results: {{save_results}}\n\n## About This Benchmark\n\n**Focus**: Multi-term query performance (Boolean AND, OR, mixed)\n**Purpose**: Deep dive into multi-term query optimization\n**Dataset**: Reuters-21578 (21,578 documents)\n**Specialization**: Tests various term counts and Boolean operators\n\n### Why Multi-Term Query Benchmarking?\n\nMulti-term queries are:\n- **Most common**: Real-world queries often have 2-5 terms\n- **Most complex**: Require intersection/union operations\n- **WAND-critical**: Early termination is most valuable for OR queries\n- **Performance-sensitive**: Can be 10-100x slower than single-term\n- **Optimization target**: Where most performance gains can be achieved\n\n## Query Test Matrix\n\n### Boolean AND Queries (Intersection)\n\n**2-term AND**:\n- `oil AND price` (high frequency both)\n- `trade AND export` (medium frequency)\n- `economic AND policy` (lower frequency)\n\n**3-term AND**:\n- `oil AND price AND market`\n- `trade AND export AND agreement`\n- `economic AND policy AND government`\n\n**5-term AND**:\n- `oil AND price AND market AND crude AND barrel`\n- `trade AND export AND import AND deficit AND surplus`\n\n**10-term AND** (stress test):\n- `oil AND price AND market AND crude AND barrel AND production AND opec AND energy AND petroleum AND industry`\n\n### Boolean OR Queries (Union)\n\n**2-term OR**:\n- `oil OR petroleum` (similar terms)\n- `dollar OR currency` (related terms)\n- `trade OR export` (related concepts)\n\n**3-term OR**:\n- `oil OR petroleum OR energy`\n- `dollar OR currency OR exchange`\n- `trade OR export OR import`\n\n**5-term OR**:\n- `oil OR trade OR market OR price OR dollar`\n- `bank OR financial OR credit OR loan OR mortgage`\n- `stock OR share OR equity OR trading OR investor`\n\n**10-term OR** (WAND stress test):\n- `oil OR trade OR market OR price OR dollar OR economy OR bank OR stock OR government OR company`\n\n### Mixed Boolean Queries\n\n**AND with OR** (nested):\n- `(oil OR petroleum) AND (price OR cost)`\n- `(trade OR export) AND (deficit OR surplus)`\n- `(bank OR financial) AND (crisis OR problem)`\n\n**OR with AND** (nested):\n- `(oil AND price) OR (trade AND deficit)`\n- `(dollar AND exchange) OR (currency AND rate)`\n\n**Complex nested**:\n- `(oil OR petroleum) AND (price OR cost) AND market`\n- `((trade OR export) AND deficit) OR ((import OR goods) AND surplus)`\n\n## Term Frequency Analysis\n\n### High Frequency Terms (>1000 docs)\n- `market`, `price`, `trade`, `bank`, `government`\n- Expected: Fast for AND, challenging for OR\n\n### Medium Frequency Terms (100-1000 docs)\n- `oil`, `dollar`, `export`, `stock`, `economic`\n- Expected: Balanced performance\n\n### Low Frequency Terms (<100 docs)\n- `petroleum`, `deficit`, `mortgage`, `equity`\n- Expected: Fast for AND (early termination), variable for OR\n\n### Mixed Frequency Queries\n- Combine high + low frequency terms\n- Test optimizer's frequency-based ordering\n\n## Performance Targets\n\n### Boolean AND Queries (P99 latency)\n- **2-term**: <2ms (2,000 μs)\n- **3-term**: <3ms (3,000 μs)\n- **5-term**: <5ms (5,000 μs)\n- **10-term**: <10ms (10,000 μs)\n\n### Boolean OR Queries (P99 latency)\n- **2-term**: <3ms (3,000 μs)\n- **3-term**: <5ms (5,000 μs)\n- **5-term**: <8ms (8,000 μs)\n- **10-term**: <15ms (15,000 μs)\n\n### Mixed Queries (P99 latency)\n- **Simple mixed**: <5ms (5,000 μs)\n- **Complex mixed**: <10ms (10,000 μs)\n\n### Scalability\n- **Throughput**: Should scale sub-linearly with term count\n- **WAND effectiveness**: OR queries should not scale linearly\n- **Memory**: Should not grow excessively with term count\n\n## Execution Procedure\n\n### Step 1: Build Benchmark (if build=true)\n\n```bash\ncd /home/ubuntu/diagon/build\n\n# Build or use existing multi-term benchmark\n# If no dedicated multi-term benchmark exists, use ReutersBenchmark\nmake ReutersBenchmark -j8\n```\n\n### Step 2: Prepare Query Set\n\nGenerate query set based on {{query_type}} and {{term_counts}}:\n\n```python\nquery_set = []\n\nif query_type in ['and', 'all']:\n    if term_counts in ['small', 'all']:\n        query_set.extend(TWO_TERM_AND_QUERIES)\n        query_set.extend(THREE_TERM_AND_QUERIES)\n    if term_counts in ['medium', 'all']:\n        query_set.extend(FIVE_TERM_AND_QUERIES)\n    if term_counts in ['large', 'all']:\n        query_set.extend(TEN_TERM_AND_QUERIES)\n\nif query_type in ['or', 'all']:\n    if term_counts in ['small', 'all']:\n        query_set.extend(TWO_TERM_OR_QUERIES)\n        query_set.extend(THREE_TERM_OR_QUERIES)\n    if term_counts in ['medium', 'all']:\n        query_set.extend(FIVE_TERM_OR_QUERIES)\n    if term_counts in ['large', 'all']:\n        query_set.extend(TEN_TERM_OR_QUERIES)\n\nif query_type in ['mixed', 'all']:\n    query_set.extend(MIXED_BOOLEAN_QUERIES)\n```\n\n### Step 3: Index Reuters Dataset\n\n```bash\n# Clean and index\nrm -rf /tmp/diagon_reuters_index\ncd /home/ubuntu/diagon/build/benchmarks\n\n# Run indexing phase only\n# (most benchmarks do this automatically)\n```\n\n### Step 4: Run Multi-Term Queries\n\nFor each query in query_set:\n\n```bash\n# Run {{iterations}} iterations\n# Measure P50, P95, P99 latencies\n# Count hits\n# Track memory usage (if possible)\n\necho \"Query: $QUERY\"\necho \"  Term count: $TERM_COUNT\"\necho \"  Type: $QUERY_TYPE\"\necho \"  Iterations: {{iterations}}\"\necho \"  P50 latency: $P50_US μs\"\necho \"  P95 latency: $P95_US μs\"\necho \"  P99 latency: $P99_US μs\"\necho \"  Hits: $HIT_COUNT\"\necho \"  Status: $STATUS\"\n```\n\n### Step 5: Analyze Results\n\nPerform detailed analysis:\n\n**Scalability Analysis**:\n```\n# Check if latency grows sub-linearly with term count\n# For AND queries: should be nearly linear or sub-linear\n# For OR queries with WAND: should be sub-linear\n# For OR queries without WAND: would be linear or super-linear\n```\n\n**WAND Effectiveness**:\n```\n# For OR queries, calculate WAND benefit:\n# Without WAND: O(sum of all posting lists)\n# With WAND: O(K + early termination)\n# Measure: documents scanned vs total documents in posting lists\n```\n\n**Frequency Impact**:\n```\n# Compare high-freq vs low-freq term queries\n# High-freq AND: should be fast (early termination)\n# High-freq OR: challenging (many candidates)\n# Low-freq AND: very fast (small intersection)\n# Low-freq OR: moderate (fewer candidates)\n```\n\n### Step 6: Generate Report\n\n**CRITICAL**: Follow `.claude/BENCHMARK_REPORT_TEMPLATE_V2.md`\n\nReport MUST include:\n\n**1. Executive Summary**:\n- Overall result (PASS/PARTIAL/FAIL)\n- Key findings for multi-term performance\n- Best performing query types\n- Worst performing query types\n- Critical issues\n\n**2. Test Environment**:\n- Standard environment details\n- Multi-term benchmark configuration\n- Query set details\n\n**3. Multi-Term Query Performance** (Enhanced section):\n\n**Boolean AND Performance**:\n| Term Count | Sample Query | Hits | P50 (μs) | P95 (μs) | P99 (μs) | Target | Status |\n|------------|--------------|------|----------|----------|----------|--------|--------|\n| 2 | oil AND price | [n] | [val] | [val] | [val] | <2ms | [✅/❌] |\n| 3 | oil AND price AND market | [n] | [val] | [val] | [val] | <3ms | [✅/❌] |\n| 5 | [5-term query] | [n] | [val] | [val] | [val] | <5ms | [✅/❌] |\n| 10 | [10-term query] | [n] | [val] | [val] | [val] | <10ms | [✅/❌] |\n\n**Boolean OR Performance**:\n| Term Count | Sample Query | Hits | P50 (μs) | P95 (μs) | P99 (μs) | Target | Status |\n|------------|--------------|------|----------|----------|----------|--------|--------|\n| 2 | oil OR petroleum | [n] | [val] | [val] | [val] | <3ms | [✅/❌] |\n| 3 | oil OR petroleum OR energy | [n] | [val] | [val] | [val] | <5ms | [✅/❌] |\n| 5 | [5-term OR] | [n] | [val] | [val] | [val] | <8ms | [✅/❌] |\n| 10 | [10-term OR] | [n] | [val] | [val] | [val] | <15ms | [✅/❌] |\n\n**Mixed Boolean Performance**:\n| Query Type | Sample Query | Hits | P99 (μs) | Target | Status |\n|------------|--------------|------|----------|--------|--------|\n| AND+OR | (oil OR petroleum) AND price | [n] | [val] | <5ms | [✅/❌] |\n| Complex | [complex query] | [n] | [val] | <10ms | [✅/❌] |\n\n**4. Scalability Analysis**:\n\n**Latency vs Term Count**:\n- Plot or table showing latency growth with term count\n- Linear, sub-linear, or super-linear?\n- Compare AND vs OR scaling\n\n**WAND Effectiveness** (for OR queries):\n- Documents scanned vs total posting list size\n- Early termination rate\n- Performance benefit vs non-WAND baseline\n\n**5. Performance Analysis**:\n- Which query types meet targets?\n- Which need optimization?\n- WAND working effectively?\n- Frequency-based optimization working?\n\n**6. Detailed Comparison**:\n- Compare with baseline (if available)\n- Compare with targets\n- Identify regressions\n\n**7-10**: Standard report sections (Issues, Recommendations, Raw Data, Reproducibility)\n\n**Report Format**:\n- Save to: `/home/ubuntu/diagon/benchmark_results/diagon_multiterm_[YYYYMMDD_HHMMSS].md`\n- Follow template structure exactly\n\n## Key Metrics to Report\n\n### Performance Metrics\n1. **Latency by term count**: Does it scale well?\n2. **Latency by query type**: AND vs OR vs mixed\n3. **Latency by frequency**: High vs low frequency terms\n4. **WAND effectiveness**: For OR queries\n5. **Memory usage**: Peak memory during query\n\n### Scalability Metrics\n1. **Growth rate**: Linear, sub-linear, super-linear?\n2. **WAND benefit**: Performance gain from early termination\n3. **Frequency optimization**: Does low-freq AND terminate early?\n\n### Quality Metrics\n1. **Correctness**: Hit counts reasonable?\n2. **Stability**: Low variance across iterations?\n3. **Consistency**: Similar queries have similar performance?\n\n## Success Criteria\n\nA successful multi-term benchmark must:\n- ✅ All term counts tested (2, 3, 5, 10)\n- ✅ All query types tested (AND, OR, mixed)\n- ✅ Latencies meet targets\n- ✅ Sub-linear scaling demonstrated (especially for OR with WAND)\n- ✅ No crashes or errors\n- ✅ Results stable across iterations\n- ✅ Complete report generated\n\n## Error Handling\n\n### Query Timeout\n```\n⚠️ Query timeout: [query]\nLatency: >[timeout]ms\nAction: Flag as performance issue, investigate\n```\n\n### Unexpected Results\n```\n⚠️ Unexpected hit count: [query]\nExpected: ~[estimate]\nActual: [count]\nAction: Verify query correctness\n```\n\n### Poor Scaling\n```\n❌ Linear or super-linear scaling detected for OR queries!\nThis suggests WAND is not working effectively.\nAction: Investigate WAND implementation\n```\n\n## Use Cases\n\n### Optimization Validation\n- Before: Benchmark multi-term queries\n- Optimize: WAND, frequency ordering, etc.\n- After: Benchmark again\n- Compare: Measure improvement\n\n### Regression Detection\n- Baseline: Save multi-term metrics\n- After changes: Re-benchmark\n- Compare: Detect any regressions\n\n### Scalability Testing\n- Test: Various term counts (2-10)\n- Analyze: Growth rate\n- Validate: Sub-linear scaling\n\n### WAND Effectiveness\n- Test: OR queries with different term counts\n- Measure: Documents scanned\n- Validate: Early termination working\n\nNow execute the multi-term query benchmark following these steps."
}
