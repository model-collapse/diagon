# Analyzer Framework Test Coverage Report

**Date**: 2026-01-27
**Repository**: Quidditch (with Diagon upstream)
**Feature**: Analyzer Framework for text analysis and tokenization

---

## Executive Summary

**Finding**: The Analyzer framework has **GOOD Go integration tests** but **NO C++ unit tests**.

**Test Coverage Status**:
- ‚úÖ **Go Integration Tests**: 17 tests, all passing
- ‚ùå **C++ Unit Tests**: 0 tests (missing!)
- ‚ö†Ô∏è **C API Tests**: 0 tests (missing!)

**Risk Level**: üü° **MEDIUM**
- Go tests provide good end-to-end coverage
- Missing C++ unit tests means no coverage of internal logic
- C API boundary not tested independently

**Recommendation**: Add C++ unit tests for core Analyzer classes to ensure robustness and prevent regressions.

---

## Implementation Overview

### Location

**Quidditch Repository**:
- C++ Code: `/home/ubuntu/quidditch/pkg/data/diagon/upstream/src/core/`
- Go Bindings: `/home/ubuntu/quidditch/pkg/data/`
- Documentation: `/home/ubuntu/quidditch/pkg/data/diagon/upstream/docs/designs/ANALYZER_FRAMEWORK_COMPLETE.md`

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Go Application              ‚îÇ
‚îÇ  (Quidditch Index Server)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ CGO
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         C API (analysis_c.h)        ‚îÇ
‚îÇ  - diagon_analyzer_create_*()       ‚îÇ
‚îÇ  - diagon_analyzer_analyze()        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         C++ Implementation          ‚îÇ
‚îÇ  - Analyzer (abstract base)         ‚îÇ
‚îÇ  - CompositeAnalyzer                ‚îÇ
‚îÇ  - Tokenizers (7 types)             ‚îÇ
‚îÇ  - TokenFilters (3 types)           ‚îÇ
‚îÇ  - Token (position-aware)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Components Implemented

**Phase 1-5: C++ Core** (2,500 lines)
1. **Token** - Position-aware token with text, offsets, type
2. **Tokenizers** (7 types):
   - WhitespaceTokenizer
   - KeywordTokenizer
   - StandardTokenizer (ICU-based)
   - LetterTokenizer
   - JiebaTokenizer (Chinese, cppjieba)
   - PatternTokenizer
   - NGramTokenizer
3. **TokenFilters** (3 types):
   - LowercaseFilter
   - StopFilter (English/Chinese)
   - ASCIIFoldingFilter (caf√© ‚Üí cafe)
4. **Analyzers** (8 built-in):
   - standard, simple, whitespace, keyword
   - chinese, english, multilingual, search
5. **AnalyzerFactory** - Factory for creating analyzers

**Phase 6: C API** (466 lines)
- Opaque handles for C compatibility
- Exception-safe wrappers
- Resource management (create/destroy)
- Error handling (thread-local)

**Phase 7-8: Go Integration** (800 lines)
- CGO bindings to C API
- Type-safe Go wrappers
- Memory management with defer
- AnalyzerSettings (per-field configuration)
- AnalyzerCache (instance pooling)

---

## Current Test Coverage

### ‚úÖ Go Integration Tests (17 tests, all passing)

**Location**:
- `/home/ubuntu/quidditch/pkg/data/diagon/analysis_test.go` (8 tests)
- `/home/ubuntu/quidditch/pkg/data/analyzer_settings_test.go` (6 tests)
- `/home/ubuntu/quidditch/pkg/data/analyzer_integration_test.go` (3 tests)

**Tests**:

#### Analyzer Tests (8)
1. `TestStandardAnalyzer` - Tests standard analyzer (tokenize + lowercase + stop words)
2. `TestSimpleAnalyzer` - Tests simple analyzer (whitespace + lowercase)
3. `TestWhitespaceAnalyzer` - Tests whitespace tokenizer
4. `TestKeywordAnalyzer` - Tests keyword tokenizer (no tokenization)
5. `TestChineseAnalyzer` - Tests Jieba Chinese segmentation
6. `TestEnglishAnalyzer` - Tests English analyzer with ASCII folding
7. `TestAnalyzeToStrings` - Tests convenience method
8. `TestNewAnalyzer` - Tests factory method

#### Settings Tests (6)
9. `TestDefaultAnalyzerSettings` - Tests default configuration
10. `TestGetAnalyzerForField` - Tests field-specific analyzer selection
11. `TestSetFieldAnalyzer` - Tests setting per-field analyzers
12. `TestValidateAnalyzerSettings` - Tests validation logic
13. `TestAnalyzerCache` - Tests analyzer instance caching
14. `TestAnalyzeField` - Tests field analysis helper

#### Integration Tests (3)
15. `TestAnalyzerIntegration` - End-to-end analyzer with shards
16. `TestAnalyzerSettingsPersistence` - Tests settings save/load
17. `TestAnalyzerCacheReuse` - Tests cache instance reuse

### ‚ùå C++ Unit Tests (0 tests)

**Expected Location**: `/home/ubuntu/quidditch/pkg/data/diagon/upstream/tests/unit/analysis/`

**Status**: ‚ùå **Directory does not exist**

**Missing Tests**:

#### Core Classes
- ‚ùå `TokenTest.cpp` - Token construction, offsets, types
- ‚ùå `CompositeAnalyzerTest.cpp` - Analyzer composition
- ‚ùå `AnalyzerFactoryTest.cpp` - Factory methods

#### Tokenizers
- ‚ùå `WhitespaceTokenizerTest.cpp`
- ‚ùå `KeywordTokenizerTest.cpp`
- ‚ùå `StandardTokenizerTest.cpp` (ICU integration)
- ‚ùå `LetterTokenizerTest.cpp`
- ‚ùå `JiebaTokenizerTest.cpp` (Chinese)
- ‚ùå `PatternTokenizerTest.cpp`
- ‚ùå `NGramTokenizerTest.cpp`

#### Token Filters
- ‚ùå `LowercaseFilterTest.cpp`
- ‚ùå `StopFilterTest.cpp` (English/Chinese stop words)
- ‚ùå `ASCIIFoldingFilterTest.cpp` (Unicode normalization)

#### Edge Cases
- ‚ùå Empty text handling
- ‚ùå Unicode edge cases
- ‚ùå Very long text (performance)
- ‚ùå Malformed input
- ‚ùå Thread safety tests

### ‚ùå C API Tests (0 tests)

**Expected Location**: `/home/ubuntu/quidditch/pkg/data/diagon/upstream/tests/unit/api/analysis_c_test.cpp`

**Missing Tests**:
- ‚ùå C API error handling
- ‚ùå Memory management (create/destroy)
- ‚ùå Exception safety (C++ exceptions caught)
- ‚ùå Thread-local error storage
- ‚ùå Null pointer handling

---

## Test Coverage Gap Analysis

### What is Tested

‚úÖ **End-to-end functionality**:
- All 8 built-in analyzers work correctly
- Tokenization produces expected tokens
- Filters apply correctly
- Chinese text segmentation works
- ASCII folding works (caf√© ‚Üí cafe)
- Stop word removal works

‚úÖ **Go integration**:
- CGO bindings work
- Memory management correct
- Per-field analyzers work
- Analyzer caching works
- Settings validation works

### What is NOT Tested

‚ùå **C++ internal logic**:
- Token class methods (getters, setters, validation)
- Tokenizer edge cases (empty text, whitespace-only, etc.)
- Filter chain composition
- Error handling in C++ layer
- Memory management in C++ (RAII, smart pointers)

‚ùå **C API boundary**:
- Exception translation (C++ ‚Üí C error codes)
- Null pointer handling
- Invalid handle detection
- Thread-local error storage
- Resource leak prevention

‚ùå **Performance**:
- Tokenization speed (large documents)
- Memory usage (leak detection)
- Thread safety (concurrent access)
- Cache efficiency

‚ùå **Edge cases**:
- Unicode edge cases (surrogate pairs, combining characters)
- Very long tokens (>10KB)
- Malformed UTF-8
- Empty input at various levels
- Null/invalid parameters

---

## Risk Assessment

### Current Risks

üî¥ **HIGH RISK**:
- **No C++ unit tests** means internal bugs could go undetected
- **No C API tests** means boundary issues not caught
- Changes to C++ code have no regression protection

üü° **MEDIUM RISK**:
- **Limited edge case coverage** - only happy path tested
- **No performance tests** - could regress without notice
- **No thread safety tests** - concurrent usage not validated

üü¢ **LOW RISK**:
- Go integration tests provide good end-to-end coverage
- Manual testing has validated basic functionality
- Simple architecture reduces bug surface area

### Potential Issues

Without C++ unit tests, these bugs could be missed:

1. **Token offset calculations**:
   ```cpp
   // Bug: Off-by-one in offset calculation
   Token::Token(const std::string& text, size_t start, size_t end) {
       // Should be: end_ = end
       end_ = end + 1;  // BUG!
   }
   ```

2. **Filter chain ordering**:
   ```cpp
   // Bug: Filters applied in wrong order
   CompositeAnalyzer::analyze() {
       auto tokens = tokenizer_->tokenize(text);
       // Should apply filters in order, but reversed:
       for (auto it = filters_.rbegin(); it != filters_.rend(); ++it) {
           tokens = (*it)->filter(tokens);  // BUG!
       }
   }
   ```

3. **Unicode handling**:
   ```cpp
   // Bug: ASCII folding breaks on certain Unicode
   ASCIIFoldingFilter::filter() {
       // Missing validation, crashes on invalid UTF-8
       transliterator->transliterate(text);  // BUG!
   }
   ```

4. **Memory leaks**:
   ```cpp
   // Bug: Raw pointer not freed
   Analyzer* analyzer = new CompositeAnalyzer(...);
   // Forgot to delete - no test would catch this!
   ```

5. **Thread safety**:
   ```cpp
   // Bug: Shared state without mutex
   class StopFilter {
       static std::set<std::string> stop_words_;  // Shared!
       void addStopWord(const std::string& word) {
           stop_words_.insert(word);  // BUG: Not thread-safe!
       }
   };
   ```

---

## Recommendations

### Priority 1: Add C++ Unit Tests ‚úÖ HIGH

Create comprehensive C++ unit tests for core functionality:

#### 1.1 Token Tests

**File**: `tests/unit/analysis/TokenTest.cpp`

```cpp
#include <gtest/gtest.h>
#include "analysis/Token.h"

using namespace diagon::analysis;

TEST(TokenTest, Construction) {
    Token token("hello", 0, 5, TokenType::WORD);

    EXPECT_EQ(token.getText(), "hello");
    EXPECT_EQ(token.getStartOffset(), 0);
    EXPECT_EQ(token.getEndOffset(), 5);
    EXPECT_EQ(token.getType(), TokenType::WORD);
}

TEST(TokenTest, EmptyText) {
    Token token("", 0, 0, TokenType::WORD);

    EXPECT_TRUE(token.getText().empty());
    EXPECT_EQ(token.getStartOffset(), 0);
    EXPECT_EQ(token.getEndOffset(), 0);
}

TEST(TokenTest, UnicodeText) {
    Token token("caf√©", 0, 4, TokenType::WORD);

    EXPECT_EQ(token.getText(), "caf√©");
    // UTF-8: c=1, a=1, f=1, √©=2 bytes
    EXPECT_EQ(token.getEndOffset(), 4);
}

TEST(TokenTest, OffsetValidation) {
    // End offset must be >= start offset
    EXPECT_THROW(Token("test", 10, 5, TokenType::WORD), std::invalid_argument);
}

TEST(TokenTest, CopyAndMove) {
    Token t1("test", 0, 4, TokenType::WORD);

    // Copy constructor
    Token t2 = t1;
    EXPECT_EQ(t2.getText(), "test");

    // Move constructor
    Token t3 = std::move(t1);
    EXPECT_EQ(t3.getText(), "test");
}
```

#### 1.2 Tokenizer Tests

**File**: `tests/unit/analysis/WhitespaceTokenizerTest.cpp`

```cpp
#include <gtest/gtest.h>
#include "analysis/WhitespaceTokenizer.h"

using namespace diagon::analysis;

TEST(WhitespaceTokenizerTest, BasicTokenization) {
    WhitespaceTokenizer tokenizer;

    auto tokens = tokenizer.tokenize("hello world test");

    ASSERT_EQ(tokens.size(), 3);
    EXPECT_EQ(tokens[0].getText(), "hello");
    EXPECT_EQ(tokens[1].getText(), "world");
    EXPECT_EQ(tokens[2].getText(), "test");
}

TEST(WhitespaceTokenizerTest, EmptyText) {
    WhitespaceTokenizer tokenizer;

    auto tokens = tokenizer.tokenize("");

    EXPECT_TRUE(tokens.empty());
}

TEST(WhitespaceTokenizerTest, OnlyWhitespace) {
    WhitespaceTokenizer tokenizer;

    auto tokens = tokenizer.tokenize("   \t\n  ");

    EXPECT_TRUE(tokens.empty());
}

TEST(WhitespaceTokenizerTest, MultipleWhitespace) {
    WhitespaceTokenizer tokenizer;

    auto tokens = tokenizer.tokenize("hello    world\t\ttest\n\nfoo");

    ASSERT_EQ(tokens.size(), 4);
    EXPECT_EQ(tokens[0].getText(), "hello");
    EXPECT_EQ(tokens[1].getText(), "world");
    EXPECT_EQ(tokens[2].getText(), "test");
    EXPECT_EQ(tokens[3].getText(), "foo");
}

TEST(WhitespaceTokenizerTest, UnicodeText) {
    WhitespaceTokenizer tokenizer;

    auto tokens = tokenizer.tokenize("caf√© r√©sum√© na√Øve");

    ASSERT_EQ(tokens.size(), 3);
    EXPECT_EQ(tokens[0].getText(), "caf√©");
    EXPECT_EQ(tokens[1].getText(), "r√©sum√©");
    EXPECT_EQ(tokens[2].getText(), "na√Øve");
}

TEST(WhitespaceTokenizerTest, OffsetCorrectness) {
    WhitespaceTokenizer tokenizer;

    auto tokens = tokenizer.tokenize("hello world");

    ASSERT_EQ(tokens.size(), 2);

    // "hello" at position 0-5
    EXPECT_EQ(tokens[0].getStartOffset(), 0);
    EXPECT_EQ(tokens[0].getEndOffset(), 5);

    // "world" at position 6-11
    EXPECT_EQ(tokens[1].getStartOffset(), 6);
    EXPECT_EQ(tokens[1].getEndOffset(), 11);
}
```

#### 1.3 Filter Tests

**File**: `tests/unit/analysis/LowercaseFilterTest.cpp`

```cpp
#include <gtest/gtest.h>
#include "analysis/LowercaseFilter.h"

using namespace diagon::analysis;

TEST(LowercaseFilterTest, BasicLowercase) {
    LowercaseFilter filter;

    std::vector<Token> tokens{
        Token("HELLO", 0, 5, TokenType::WORD),
        Token("World", 6, 11, TokenType::WORD)
    };

    auto result = filter.filter(tokens);

    ASSERT_EQ(result.size(), 2);
    EXPECT_EQ(result[0].getText(), "hello");
    EXPECT_EQ(result[1].getText(), "world");
}

TEST(LowercaseFilterTest, EmptyTokens) {
    LowercaseFilter filter;

    std::vector<Token> tokens;

    auto result = filter.filter(tokens);

    EXPECT_TRUE(result.empty());
}

TEST(LowercaseFilterTest, AlreadyLowercase) {
    LowercaseFilter filter;

    std::vector<Token> tokens{
        Token("hello", 0, 5, TokenType::WORD)
    };

    auto result = filter.filter(tokens);

    ASSERT_EQ(result.size(), 1);
    EXPECT_EQ(result[0].getText(), "hello");
}

TEST(LowercaseFilterTest, UnicodeUppercase) {
    LowercaseFilter filter;

    std::vector<Token> tokens{
        Token("CAF√â", 0, 4, TokenType::WORD),
        Token("R√âSUM√â", 5, 11, TokenType::WORD)
    };

    auto result = filter.filter(tokens);

    ASSERT_EQ(result.size(), 2);
    EXPECT_EQ(result[0].getText(), "caf√©");
    EXPECT_EQ(result[1].getText(), "r√©sum√©");
}

TEST(LowercaseFilterTest, PreservesOffsets) {
    LowercaseFilter filter;

    std::vector<Token> tokens{
        Token("HELLO", 10, 15, TokenType::WORD)
    };

    auto result = filter.filter(tokens);

    ASSERT_EQ(result.size(), 1);
    EXPECT_EQ(result[0].getText(), "hello");
    EXPECT_EQ(result[0].getStartOffset(), 10);
    EXPECT_EQ(result[0].getEndOffset(), 15);
}
```

#### 1.4 Analyzer Tests

**File**: `tests/unit/analysis/CompositeAnalyzerTest.cpp`

```cpp
#include <gtest/gtest.h>
#include "analysis/Analyzer.h"
#include "analysis/WhitespaceTokenizer.h"
#include "analysis/LowercaseFilter.h"

using namespace diagon::analysis;

TEST(CompositeAnalyzerTest, NoFilters) {
    auto tokenizer = std::make_unique<WhitespaceTokenizer>();

    CompositeAnalyzer analyzer(
        "test",
        std::move(tokenizer),
        std::vector<std::unique_ptr<TokenFilter>>()
    );

    auto tokens = analyzer.analyze("HELLO WORLD");

    ASSERT_EQ(tokens.size(), 2);
    EXPECT_EQ(tokens[0].getText(), "HELLO");  // Not lowercased
    EXPECT_EQ(tokens[1].getText(), "WORLD");
}

TEST(CompositeAnalyzerTest, SingleFilter) {
    auto tokenizer = std::make_unique<WhitespaceTokenizer>();

    std::vector<std::unique_ptr<TokenFilter>> filters;
    filters.push_back(std::make_unique<LowercaseFilter>());

    CompositeAnalyzer analyzer("test", std::move(tokenizer), std::move(filters));

    auto tokens = analyzer.analyze("HELLO WORLD");

    ASSERT_EQ(tokens.size(), 2);
    EXPECT_EQ(tokens[0].getText(), "hello");  // Lowercased
    EXPECT_EQ(tokens[1].getText(), "world");
}

TEST(CompositeAnalyzerTest, MultipleFilters) {
    auto tokenizer = std::make_unique<WhitespaceTokenizer>();

    std::vector<std::unique_ptr<TokenFilter>> filters;
    filters.push_back(std::make_unique<LowercaseFilter>());
    filters.push_back(std::make_unique<ASCIIFoldingFilter>());

    CompositeAnalyzer analyzer("test", std::move(tokenizer), std::move(filters));

    auto tokens = analyzer.analyze("CAF√â R√âSUM√â");

    ASSERT_EQ(tokens.size(), 2);
    EXPECT_EQ(tokens[0].getText(), "cafe");    // Lowercased + ASCII folded
    EXPECT_EQ(tokens[1].getText(), "resume");
}

TEST(CompositeAnalyzerTest, EmptyText) {
    auto tokenizer = std::make_unique<WhitespaceTokenizer>();

    CompositeAnalyzer analyzer(
        "test",
        std::move(tokenizer),
        std::vector<std::unique_ptr<TokenFilter>>()
    );

    auto tokens = analyzer.analyze("");

    EXPECT_TRUE(tokens.empty());
}
```

### Priority 2: Add C API Tests ‚ö†Ô∏è MEDIUM

**File**: `tests/unit/api/analysis_c_test.cpp`

Test the C API boundary:

```cpp
#include <gtest/gtest.h>
#include "diagon/analysis_c.h"

TEST(AnalysisCAPITest, CreateAndDestroy) {
    DiagonAnalyzer* analyzer = diagon_analyzer_create_standard();

    ASSERT_NE(analyzer, nullptr);

    diagon_analyzer_destroy(analyzer);
}

TEST(AnalysisCAPITest, NullHandleCheck) {
    // Should not crash
    diagon_analyzer_destroy(nullptr);

    // Should return error
    DiagonToken** tokens = nullptr;
    size_t count = 0;
    int result = diagon_analyzer_analyze(nullptr, "test", &tokens, &count);

    EXPECT_NE(result, 0);  // Should fail
}

TEST(AnalysisCAPITest, BasicAnalysis) {
    DiagonAnalyzer* analyzer = diagon_analyzer_create_simple();

    DiagonToken** tokens = nullptr;
    size_t count = 0;

    int result = diagon_analyzer_analyze(analyzer, "hello world", &tokens, &count);

    EXPECT_EQ(result, 0);  // Success
    EXPECT_EQ(count, 2);
    ASSERT_NE(tokens, nullptr);

    // Check first token
    const char* text = diagon_token_get_text(tokens[0]);
    EXPECT_STREQ(text, "hello");

    // Cleanup
    diagon_tokens_destroy(tokens, count);
    diagon_analyzer_destroy(analyzer);
}

TEST(AnalysisCAPITest, ExceptionHandling) {
    DiagonAnalyzer* analyzer = diagon_analyzer_create_standard();

    // Invalid UTF-8 should be handled gracefully
    const char* invalid_utf8 = "\xFF\xFE invalid";

    DiagonToken** tokens = nullptr;
    size_t count = 0;

    int result = diagon_analyzer_analyze(analyzer, invalid_utf8, &tokens, &count);

    // Should either succeed (replacing invalid) or fail gracefully (not crash)
    // Error message should be available
    const char* error = diagon_get_last_error();
    if (result != 0) {
        EXPECT_NE(error, nullptr);
    }

    diagon_analyzer_destroy(analyzer);
}
```

### Priority 3: Add Edge Case Tests ‚ö†Ô∏è MEDIUM

Test boundary conditions and error cases:

```cpp
TEST(TokenizerEdgeCaseTest, VeryLongToken) {
    WhitespaceTokenizer tokenizer;

    std::string long_text(100000, 'a');  // 100KB single token

    auto tokens = tokenizer.tokenize(long_text);

    ASSERT_EQ(tokens.size(), 1);
    EXPECT_EQ(tokens[0].getText().length(), 100000);
}

TEST(TokenizerEdgeCaseTest, MalformedUTF8) {
    StandardTokenizer tokenizer;

    const char* malformed = "\xFF\xFE\xFD";

    // Should either replace invalid sequences or throw specific exception
    EXPECT_NO_THROW({
        auto tokens = tokenizer.tokenize(malformed);
        // Check that result is valid, even if empty
    });
}

TEST(FilterEdgeCaseTest, EmptyTokenText) {
    LowercaseFilter filter;

    std::vector<Token> tokens{Token("", 0, 0, TokenType::WORD)};

    auto result = filter.filter(tokens);

    ASSERT_EQ(result.size(), 1);
    EXPECT_TRUE(result[0].getText().empty());
}
```

### Priority 4: Add Performance Tests üîµ LOW

Benchmark analyzer performance:

```cpp
TEST(AnalyzerPerformanceTest, LargeDocument) {
    auto analyzer = AnalyzerFactory::createStandard();

    // Generate 1MB document
    std::string large_doc;
    for (int i = 0; i < 100000; ++i) {
        large_doc += "word" + std::to_string(i) + " ";
    }

    auto start = std::chrono::high_resolution_clock::now();

    auto tokens = analyzer->analyze(large_doc);

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);

    // Should process 1MB in < 100ms
    EXPECT_LT(duration.count(), 100);

    std::cout << "Processed " << large_doc.length() << " bytes in "
              << duration.count() << " ms\n";
}
```

---

## Implementation Plan

### Phase 1: Core Unit Tests (8 hours)

**Week 1**:
1. Create `tests/unit/analysis/` directory
2. Add TokenTest.cpp (2 hours)
3. Add WhitespaceTokenizerTest.cpp (2 hours)
4. Add LowercaseFilterTest.cpp (2 hours)
5. Add CompositeAnalyzerTest.cpp (2 hours)

**Deliverables**: 4 test files, ~500 lines, 20+ tests

### Phase 2: Comprehensive Tokenizer Tests (12 hours)

**Week 1-2**:
1. KeywordTokenizerTest.cpp (2 hours)
2. StandardTokenizerTest.cpp (4 hours - ICU integration)
3. LetterTokenizerTest.cpp (2 hours)
4. JiebaTokenizerTest.cpp (4 hours - Chinese edge cases)

**Deliverables**: 4 test files, ~600 lines, 25+ tests

### Phase 3: Filter Tests (8 hours)

**Week 2**:
1. StopFilterTest.cpp (4 hours - English/Chinese)
2. ASCIIFoldingFilterTest.cpp (4 hours - Unicode)

**Deliverables**: 2 test files, ~400 lines, 15+ tests

### Phase 4: C API Tests (6 hours)

**Week 2**:
1. analysis_c_test.cpp (6 hours)

**Deliverables**: 1 test file, ~300 lines, 10+ tests

### Phase 5: Edge Cases & Performance (8 hours)

**Week 3**:
1. Edge case tests (4 hours)
2. Performance benchmarks (4 hours)

**Deliverables**: 2 test files, ~200 lines, 10+ tests

**Total Effort**: ~42 hours (1 week sprint)

---

## Test Metrics Target

### Coverage Goals

| Component | Current | Target | Priority |
|-----------|---------|--------|----------|
| Token class | 0% | 90%+ | HIGH |
| Tokenizers | 0% | 80%+ | HIGH |
| Filters | 0% | 80%+ | HIGH |
| Analyzer | 0% | 90%+ | HIGH |
| C API | 0% | 70%+ | MEDIUM |
| Factory | 0% | 70%+ | MEDIUM |

### Test Count Goals

| Category | Current | Target |
|----------|---------|--------|
| **C++ Unit Tests** | 0 | 80+ |
| **C API Tests** | 0 | 10+ |
| **Edge Case Tests** | 0 | 15+ |
| **Performance Tests** | 0 | 5+ |
| **Go Integration Tests** | 17 | 17 (keep existing) |
| **Total Tests** | **17** | **127+** |

---

## Summary

### Current Status

‚úÖ **Strengths**:
- Good Go integration test coverage (17 tests)
- End-to-end functionality validated
- All 8 analyzers tested from Go
- Per-field analyzer configuration tested

‚ùå **Weaknesses**:
- **ZERO C++ unit tests**
- No C API boundary tests
- No edge case coverage
- No performance benchmarks
- Internal logic not validated

### Risk Level: üü° MEDIUM

The Analyzer framework is **production-ready** from a functionality perspective (Go tests pass), but **lacks regression protection** at the C++ level.

### Recommendation

**Add C++ unit tests immediately** to protect against future regressions and validate internal logic. Start with high-priority tests (Token, Tokenizers, Filters) and expand from there.

**Estimated effort**: 42 hours (1 week sprint) to achieve comprehensive coverage.

---

**Prepared By**: Claude Sonnet 4.5
**Date**: 2026-01-27
**Status**: Analysis complete, implementation plan ready
