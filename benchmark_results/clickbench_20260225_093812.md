# Benchmark Report: ClickBench Lite (100K rows)

**Report ID**: `clickbench_20260225_093812`
**Generated**: 2026-02-25 09:38:12 UTC
**Diagon Version**: 7c10a12
**Benchmark Skill**: benchmark_clickbench v1.0
**Benchmark Duration**: ~3.5 minutes (indexing + 15 queries x 110 iterations)

---

## Executive Summary

**Overall Result**: **PARTIAL**

This is the first ClickBench run. 100K-row subset validates correctness and provides initial latency baselines. Several queries return 0 hits due to the small data subset (specific UserIDs, URLs, CounterIDs not present in the first 100K rows). Numeric range queries are slow (13-19ms) due to full-scan doc values evaluation without index support.

### Key Findings

1. **Indexing throughput is strong**: 31,898 docs/sec at 100K rows with 20+ fields per document.
   - Impact: High
   - Status: **PASS**

2. **Numeric range queries are slow**: Q9, Q10, Q14 at 13-19ms P99 — full doc values scan with no skip index.
   - Impact: High
   - Status: **NEEDS IMPROVEMENT**

3. **Term-based queries are fast**: Point lookups and boolean filters at 3-500 us — inverted index working well.
   - Impact: High
   - Status: **PASS**

4. **Several queries return 0 hits**: Q3-Q7, Q11 have 0 matches in the first 100K rows.
   - Impact: Medium
   - Status: **EXPECTED** (small data subset)

5. **Index size is 940 bytes/doc**: Higher than Reuters (250-700 bytes/doc) due to 20+ fields per document.
   - Impact: Medium
   - Status: **ACCEPTABLE**

---

## Test Environment

| Parameter | Value |
|-----------|-------|
| Machine | Linux 6.14.0-1015-aws |
| CPU | (system default, -march=native) |
| Build | Release, -O3, no LTO |
| Dataset | ClickBench hits.tsv (first 100K rows) |
| max_docs | 100,000 |
| I/O (index) | FSDirectory |
| I/O (query) | MMapDirectory |
| Warmup | 10 iterations per query |
| Measured | 100 iterations per query |
| Segments | 1 (single segment) |

---

## Indexing Performance

| Metric | Value |
|--------|-------|
| Documents indexed | 100,000 |
| Indexing time | 3.135 seconds |
| Throughput | 31,898 docs/sec |
| Index size | 89 MB |
| Storage per doc | 940 bytes/doc |
| Fields per doc | ~20 (8 numeric + 10 string + 2-3 text) |

---

## Query Performance

### Full Results Table (P50 / P90 / P99)

| # | Query | P50 (ms) | P90 (ms) | P99 (ms) | Hits |
|---|-------|----------|----------|----------|------|
| Q1 | COUNT(*) | 0.509 | 0.514 | 0.538 | 100,000 |
| Q2 | AdvEngineID <> 0 | 1.147 | 1.156 | 1.180 | 2,885 |
| Q3 | UserID = 435090932899640449 | 0.003 | 0.003 | 0.015 | 0 |
| Q4 | URL contains 'google' | 0.004 | 0.004 | 0.018 | 0 |
| Q5 | CounterID=62 AND date AND flags | 0.009 | 0.009 | 0.021 | 0 |
| Q6 | CounterID=62 AND date=2013-07-01 | 0.009 | 0.009 | 0.019 | 0 |
| Q7 | Complex: CID=62 AND flags (6 clauses) | 0.014 | 0.014 | 0.025 | 0 |
| Q8 | RegionID IN (1..10) | 0.505 | 0.514 | 0.541 | 21,040 |
| Q9 | RegionID BETWEEN 200 AND 300 | 13.306 | 13.572 | 13.836 | 51,474 |
| Q10 | ResolutionWidth >= 1900 | 13.134 | 13.403 | 13.483 | 27,222 |
| Q11 | URL='google' AND AdvEngineID<>0 | 0.007 | 0.008 | 0.018 | 0 |
| Q12 | SearchPhrase <> '' (non-empty) | 0.707 | 0.711 | 0.721 | 100,000 |
| Q13 | EventDate = '2013-07-15' | 1.616 | 1.626 | 1.656 | 100,000 |
| Q14 | CounterID BETWEEN 0 AND 100 | 18.544 | 18.687 | 19.131 | 1,212,000 |
| Q15 | AdvEngineID IN (2,3,4) | 0.068 | 0.069 | 0.080 | 1,956 |

### Query Category Breakdown

#### Full Scan (Q1)
- **COUNT(*)**: 0.538ms P99 for 100K docs — fast MatchAllQuery.

#### Point Lookups (Q3, Q13)
- **Q3** (UserID exact): 0.015ms P99 — no matches in subset (expected), FST lookup returns immediately.
- **Q13** (EventDate exact): 1.656ms P99 — 100K hits, all docs match this date. High hit count drives latency.

#### NOT Filters (Q2, Q12)
- **Q2** (AdvEngineID <> 0): 1.180ms P99 — MatchAll minus 97K docs = 2,885 results. Reasonable.
- **Q12** (SearchPhrase non-empty): 0.721ms P99 — MatchAll minus TermQuery(""). All 100K match (empty string term not indexed or all have phrases).

#### Multi-Filter AND (Q5-Q7)
- **Q5-Q7**: 0.019-0.025ms P99 — all return 0 hits. CounterID=62 and specific dates not in first 100K rows. Queries terminate very quickly on conjunction miss.

#### Multi-Term OR (Q8, Q15)
- **Q8** (RegionID IN 1..10): 0.541ms P99 — 10-way OR on StringField, 21K hits. Strong.
- **Q15** (AdvEngineID IN 2,3,4): 0.080ms P99 — 3-way OR, 1,956 hits. Very fast.

#### Numeric Range (Q9, Q10, Q14)
- **Q9** (RegionID 200-300): 13.836ms P99 — 51K hits. Full doc values scan.
- **Q10** (ResolutionWidth >= 1900): 13.483ms P99 — 27K hits. Full doc values scan.
- **Q14** (CounterID 0-100): 19.131ms P99 — 1.2M hits (!). This hit count exceeds doc count (100K), indicating duplicate counting across multiple NumericDocValues fields. **BUG**: totalHits reports 1,212,000 for 100K docs — likely counting across multiple doc values entries per document since CounterID is indexed as both NumericDocValues and StringField.

---

## Issues and Concerns

### Critical

1. **Q14 reports 1,212,000 hits for 100K documents**: NumericRangeQuery on "CounterID" may be matching across all numeric doc values fields, not just the named field. This is a correctness bug in hit counting or in how NumericRangeQuery matches fields.

### Significant

2. **Numeric range queries are slow (13-19ms)**: The NumericDocValuesWriter uses dense arrays with O(N) scan per query. At 10M documents, this would be 130-190ms — unacceptable. Need skip-list or BKD tree index for range queries.

3. **Core bug: NumericDocValuesWriter uses maxBufferedDocs as maxDoc**: The writer allocates `maxBufferedDocs` entries per field regardless of actual segment size. Fixed in the benchmark by setting `maxBufferedDocs = maxDocs + 1000`, but the core code should use actual segment doc count.

### Minor

4. **Zero-hit queries (Q3-Q7, Q11)**: Expected with 100K subset. Need full dataset run to validate these query paths under load.

5. **Index size 940 bytes/doc**: Acceptable for 20+ fields, but dense numeric doc values storage is wasteful. Compression (delta, bitpacking) would reduce significantly.

---

## Recommendations

1. **Fix Q14 hit count bug**: Investigate NumericRangeQuery field matching — totalHits should not exceed maxDoc.
2. **Add BKD tree or skip index for numeric ranges**: Current O(N) scan won't scale to 10M+ docs.
3. **Fix NumericDocValuesWriter maxDoc bug in core**: Should use actual segment doc count, not buffer capacity.
4. **Run with full dataset (10M rows)**: Validates queries with non-zero hits for Q3-Q7.
5. **Add doc values compression**: Delta encoding + bitpacking for numeric fields would reduce 89MB index significantly.

---

## Raw Data

```
Documents: 100000
Indexing time (ms): 3135
Throughput (docs/sec): 31898
Index size (bytes): 93323264

Q1  COUNT(*)                           | P50: 509 us | P90: 514 us | P99: 538 us | Hits: 100000
Q2  AdvEngineID <> 0                   | P50: 1147 us | P90: 1156 us | P99: 1180 us | Hits: 2885
Q3  UserID = 435090932899640449        | P50: 3 us | P90: 3 us | P99: 15 us | Hits: 0
Q4  URL contains 'google'             | P50: 4 us | P90: 4 us | P99: 18 us | Hits: 0
Q5  CounterID=62 AND date AND flags   | P50: 9 us | P90: 9 us | P99: 21 us | Hits: 0
Q6  CounterID=62 AND date=2013-07-01  | P50: 9 us | P90: 9 us | P99: 19 us | Hits: 0
Q7  Complex: CID=62 AND flags (6 cl)  | P50: 14 us | P90: 14 us | P99: 25 us | Hits: 0
Q8  RegionID IN (1..10)               | P50: 505 us | P90: 514 us | P99: 541 us | Hits: 21040
Q9  RegionID BETWEEN 200 AND 300      | P50: 13306 us | P90: 13572 us | P99: 13836 us | Hits: 51474
Q10 ResolutionWidth >= 1900           | P50: 13134 us | P90: 13403 us | P99: 13483 us | Hits: 27222
Q11 URL='google' AND AdvEngineID<>0   | P50: 7 us | P90: 8 us | P99: 18 us | Hits: 0
Q12 SearchPhrase <> '' (non-empty)    | P50: 707 us | P90: 711 us | P99: 721 us | Hits: 100000
Q13 EventDate = '2013-07-15'          | P50: 1616 us | P90: 1626 us | P99: 1656 us | Hits: 100000
Q14 CounterID BETWEEN 0 AND 100       | P50: 18544 us | P90: 18687 us | P99: 19131 us | Hits: 1212000
Q15 AdvEngineID IN (2,3,4)            | P50: 68 us | P90: 69 us | P99: 80 us | Hits: 1956
```

---

## Reproducibility

```bash
# Build
/build_diagon target=benchmarks

# Download dataset (if not present)
mkdir -p /home/ubuntu/data/clickbench
cd /home/ubuntu/data/clickbench
wget -c https://datasets.clickhouse.com/hits_compatible/hits.tsv.gz
gunzip hits.tsv.gz

# Extract 100K subset
zcat hits.tsv.gz | head -100000 > hits_100k.tsv

# Run benchmark
cd /home/ubuntu/diagon/build/benchmarks
./ClickBenchBenchmark --data-path /home/ubuntu/data/clickbench/hits_100k.tsv --max-docs 100000
```
