# ClickBench Benchmark Report

**Date**: 2026-02-25 10:21 UTC
**Dataset**: ClickBench hits_100k.tsv (100K rows)
**Storage**: Dual path — Lucene inverted index + Columnar store with LZ4/MinMax

## Executive Summary

First benchmark with the new **dual storage path**: Lucene for text/string queries, columnar store with LZ4-compressed granules and MinMax skip indexes for numeric range queries (Q9, Q10, Q14).

**Key result**: Columnar range queries deliver sub-millisecond latency. Q14 (CounterID BETWEEN 0 AND 100) achieves **0 microseconds P99** via pure MinMax bulk-counting — all 13 granules are fully within range, zero decompression needed.

## Test Environment

| Parameter | Value |
|-----------|-------|
| Platform | Linux 6.14.0-1015-aws (x86_64) |
| Build | Release -O3 -march=native, LTO OFF |
| Dataset | hits_100k.tsv (100,000 rows) |
| Index path | /tmp/diagon_clickbench_index |
| Columnar path | /tmp/diagon_clickbench_index_columnar |
| Granule size | 8192 rows |
| Granules per column | 13 |
| Query I/O | MMapDirectory (Lucene), in-memory (Columnar) |
| Iterations | 10 warmup + 100 measured |

## Indexing Performance

| Metric | Value |
|--------|-------|
| Documents indexed | 100,000 |
| Indexing time | 3.114 seconds |
| Throughput | 32,113 docs/sec |
| Lucene index size | 89 MB (940 bytes/doc) |
| Columnar store size | ~75 KB (3 columns, LZ4-compressed) |

Columnar store is extremely compact: 75 KB for 3 columns of 100K int64 values = ~0.75 bytes/doc (vs 940 bytes/doc for full Lucene index). LZ4 compression ratio ~10:1 on sorted-ish numeric data.

## Query Performance

### All 15 Queries (P50 / P90 / P99)

| Query | Type | P50 (ms) | P90 (ms) | P99 (ms) | Hits |
|-------|------|----------|----------|----------|------|
| Q1 COUNT(*) | Full scan | 0.509 | 0.514 | 0.535 | 100,000 |
| Q2 AdvEngineID <> 0 | NOT filter | 1.152 | 1.162 | 1.203 | 2,885 |
| Q3 UserID = specific | Point lookup | 0.003 | 0.003 | 0.004 | 0 |
| Q4 URL contains 'google' | Text search | 0.004 | 0.004 | 0.016 | 0 |
| Q5 CID=62 AND date AND flags | Multi-AND (4) | 0.009 | 0.009 | 0.021 | 0 |
| Q6 CID=62 AND date=07-01 | Multi-AND (4) | 0.009 | 0.009 | 0.020 | 0 |
| Q7 CID=62 AND 6 conditions | Complex bool (6) | 0.014 | 0.014 | 0.033 | 0 |
| Q8 RegionID IN (1..10) | Multi-OR (10) | 0.517 | 0.523 | 0.532 | 21,040 |
| **Q9 RegionID [200,300]** | **COLUMNAR** | **0.150** | **0.159** | **0.161** | **51,474** |
| **Q10 ResWidth >= 1900** | **COLUMNAR** | **0.137** | **0.146** | **0.149** | **27,222** |
| Q11 URL=google AND Adv<>0 | Text+filter | 0.008 | 0.008 | 0.008 | 0 |
| Q12 SearchPhrase <> '' | Inverted filter | 0.625 | 0.629 | 0.637 | 100,000 |
| Q13 EventDate = 2013-07-15 | Date exact | 1.627 | 1.639 | 1.650 | 100,000 |
| **Q14 CounterID [0,100]** | **COLUMNAR** | **0.000** | **0.000** | **0.000** | **100,000** |
| Q15 AdvEngineID IN (2,3,4) | Small OR (3) | 0.068 | 0.071 | 0.240 | 1,956 |

### Columnar Granule Statistics

| Query | Total | Scanned | Skipped | Bulk-counted | Notes |
|-------|-------|---------|---------|--------------|-------|
| Q9 RegionID [200,300] | 13 | 13 | 0 | 0 | All granules have partial overlap |
| Q10 ResWidth >= 1900 | 13 | 13 | 0 | 0 | All granules have partial overlap |
| Q14 CounterID [0,100] | 13 | 0 | 0 | 13 | All granules fully within range |

## Performance Analysis

### Columnar vs Previous Lucene DocValues (before this change)

Previous benchmark numbers for Q9/Q10/Q14 (Lucene NumericRangeQuery, O(N) dense scan):

| Query | Before (Lucene DocValues) | After (Columnar) | Speedup |
|-------|---------------------------|-------------------|---------|
| Q9 RegionID [200,300] | ~13.8 ms | 0.161 ms P99 | **~86x** |
| Q10 ResWidth >= 1900 | ~13.5 ms | 0.149 ms P99 | **~91x** |
| Q14 CounterID [0,100] | ~19.1 ms | <0.001 ms P99 | **>19,000x** |

Q14's extreme speedup is due to all 13 granules being fully within the [0,100] range (all CounterIDs in the 100K sample are <= 100). The MinMax check short-circuits: no decompression, no scanning, just sum of numRows.

### Query Category Performance

**Sub-microsecond** (<0.001 ms P99):
- Q14: Pure MinMax bulk-counting

**Sub-millisecond** (<1 ms P99):
- Q3, Q4, Q5, Q6, Q7, Q11: Point lookups and multi-filter AND (0.004-0.033 ms)
- Q9, Q10: Columnar range with scan (0.149-0.161 ms)
- Q8, Q15: Multi-term OR (0.240-0.532 ms)
- Q1: COUNT(*) (0.535 ms)
- Q12: Inverted filter (0.637 ms)

**Above 1 ms**:
- Q2: NOT filter (1.203 ms) — scans all docs, then excludes
- Q13: Date exact match (1.650 ms) — high cardinality field, 100K hits

### Three-Level Evaluation Effectiveness

The columnar store's three-level evaluation works as designed:
1. **SKIP**: Not triggered here because 100K rows across 13 granules means all granules contain diverse values for RegionID/ResolutionWidth. At 1M+ docs with more granules, skip rates will increase significantly.
2. **BULK-COUNT**: Q14 demonstrates this perfectly — 100% of granules bulk-counted with zero I/O.
3. **SCAN**: Q9/Q10 fall to scan level but still achieve ~0.15 ms because LZ4 decompression of 8192 int64s is extremely fast (~65KB per granule).

## Issues and Concerns

1. **Q9/Q10 zero skip rate**: At 100K docs with only 13 granules, granule-level MinMax filtering has limited opportunity to skip. At 1M+ docs (~122 granules), the skip rate should improve dramatically for narrow ranges like [200,300].

2. **Q13 latency (1.65 ms)**: EventDate='2013-07-15' matches all 100K docs (the 100K sample may all be from the same date). This is a full-match scenario where the inverted index must iterate all postings.

3. **Storage overhead**: 940 bytes/doc for 100K docs is high. The Lucene index (89 MB) is dominated by NumericDocValues dense arrays. The columnar store adds only 75 KB — negligible.

## Recommendations

1. **Run at 1M+ docs** to validate granule skip rates. Narrow range queries (Q9: RegionID [200,300]) should see significant skip rates with 122+ granules.

2. **Decompress full dataset** (`gunzip hits.tsv.gz`) to benchmark at 10M-100M scale where columnar advantages compound.

3. **Consider removing NumericDocValuesField** for columns that are now columnar-only (RegionID, ResolutionWidth, CounterID) to reduce Lucene index size.

## Raw Data

```
Documents: 100000
Indexing time (ms): 3114
Throughput (docs/sec): 32113
Index size (bytes): 93323264
Columnar files: CounterID.col(3360B) RegionID.col(36515B) ResolutionWidth.col(34981B)

Q1  COUNT(*)                          P50: 509us  P90: 514us  P99: 535us  Hits: 100000
Q2  AdvEngineID <> 0                  P50: 1152us P90: 1162us P99: 1203us Hits: 2885
Q3  UserID = 435090932899640449       P50: 3us    P90: 3us    P99: 4us    Hits: 0
Q4  URL contains 'google'             P50: 4us    P90: 4us    P99: 16us   Hits: 0
Q5  CounterID=62 AND date AND flags   P50: 9us    P90: 9us    P99: 21us   Hits: 0
Q6  CounterID=62 AND date=2013-07-01  P50: 9us    P90: 9us    P99: 20us   Hits: 0
Q7  Complex: CID=62 AND flags        P50: 14us   P90: 14us   P99: 33us   Hits: 0
Q8  RegionID IN (1..10)              P50: 517us  P90: 523us  P99: 532us  Hits: 21040
Q9  RegionID [200,300] COLUMNAR      P50: 150us  P90: 159us  P99: 161us  Hits: 51474
Q10 ResWidth >= 1900 COLUMNAR        P50: 137us  P90: 146us  P99: 149us  Hits: 27222
Q11 URL=google AND AdvEngineID<>0    P50: 8us    P90: 8us    P99: 8us    Hits: 0
Q12 SearchPhrase <> ''               P50: 625us  P90: 629us  P99: 637us  Hits: 100000
Q13 EventDate = 2013-07-15           P50: 1627us P90: 1639us P99: 1650us Hits: 100000
Q14 CounterID [0,100] COLUMNAR       P50: 0us    P90: 0us    P99: 0us    Hits: 100000
Q15 AdvEngineID IN (2,3,4)           P50: 68us   P90: 71us   P99: 240us  Hits: 1956
```

## Reproducibility

```bash
cd /home/ubuntu/diagon/build/benchmarks
./ClickBenchBenchmark --data-path /home/ubuntu/data/clickbench/hits_100k.tsv --max-docs 100000
```
