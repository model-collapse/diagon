# P0.4 StreamVByte SIMD - Analysis

**Date**: 2026-01-30
**Status**: Implemented but no measurable improvement
**Result**: No performance change (106 Œºs vs 105 Œºs baseline)

## What Was Implemented

Modified `Lucene104PostingsEnumOptimized` to use SIMD StreamVByte decoding:

1. **Before**: Scalar `decodeStreamVByte4()` reading bytes one at a time
2. **After**: Calls `StreamVByte::decode4()` which uses AVX2 SIMD

**Key Code Changes**:
```cpp
// Before (scalar):
inline void decodeStreamVByte4(uint32_t* output) {
    uint8_t control = readByteFromBatch();
    // ... read 16 bytes one at a time with loops
}

// After (SIMD):
inline void decodeStreamVByte4(uint32_t* output) {
    int bytesConsumed = util::StreamVByte::decode4(&ioBatch_[ioBatchPos_], output);
    ioBatchPos_ += bytesConsumed;
}
```

**Files Modified**:
- `src/core/include/diagon/codecs/lucene104/Lucene104PostingsReaderOptimized.h`
- `src/core/src/codecs/lucene104/Lucene104PostingsReader.cpp` (switched to optimized version)

## Performance Results

**Benchmark**: BM_TermQuerySearch/10000 (10 repetitions with warmup)

| Implementation | Median Latency | vs Baseline | Status |
|----------------|----------------|-------------|--------|
| Baseline (collector batching only) | 105 Œºs | - | Reference |
| + SIMD StreamVByte | 106 Œºs | **0% (no change)** | ‚ùì Unclear |

**Consistency**: Very stable results (0.64% coefficient of variation)

## Analysis: Why No Improvement?

### Theory 1: Decoding Not the Bottleneck

**Original profiling** showed 13.89% CPU in postings traversal, but this includes:
- Iterator state management
- Virtual function calls (`nextDoc()`)
- Buffer management
- **Actual decoding** (unknown %)

If decoding is only 20% of the 13.89%, then 3x faster decoding = 2.8% total savings = ~3 Œºs.
This might be within measurement noise.

### Theory 2: Cache Effects

With 10,000 documents:
- Compressed size: ~20-40 KB (fits in L2 cache)
- All data likely in cache after first iteration
- Memory bandwidth not the bottleneck
- SIMD benefit reduced when data is hot in cache

### Theory 3: Instruction-Level Parallelism

The scalar version with manual loop unrolling might already be well-optimized by the compiler:
- Modern CPUs have 4-way instruction-level parallelism
- The scalar loop might be executing 2-3 decodes in parallel via pipelining
- SIMD benefit reduced when scalar code is already parallel

### Theory 4: Implementation Issue

Possible issues:
- AVX2 not actually enabled (unlikely - checked CMake flags)
- `decode4_AVX2()` implementation has bugs
- I/O batch refill overhead masks benefit

## Verification

**AVX2 Status**:
- CMake: `HAVE_AVX2=1`, `HAVE_AVX2_COMPILED=TRUE`
- Flags: `-march=native` enables AVX2 on this CPU
- `StreamVByte::decode4()` should select AVX2 path

**Dataset Size**:
- 10k docs: 106 Œºs (same as baseline)
- 50k docs: 541 Œºs (no baseline comparison)

## Recommendations

### Option A: Accept and Move On (Recommended)

**Rationale**:
1. StreamVByte optimization was speculative (no profiling data on decode cost)
2. Collector batching (P0.1) already achieved 5.4% improvement
3. Other optimizations (P0.2, P0.3) have clearer ROI
4. Time spent debugging this has diminishing returns

**Action**: Keep SIMD StreamVByte (no harm), document as "implemented but unmeasured benefit"

### Option B: Deeper Investigation

**Steps**:
1. Add timing instrumentation to `decodeStreamVByte4()` vs `StreamVByte::decode4()`
2. Test with much larger dataset (1M+ docs) where cache effects are reduced
3. Profile with `perf` to see actual CPU cycles in decode
4. Verify AVX2 path is actually taken (add debug logging)

**Effort**: 1-2 days
**Expected outcome**: Might find 1-2% improvement, or confirm it's negligible

### Option C: Try Bulk Decoding

Instead of decoding 4 integers at a time, decode 128 at once:

```cpp
// In refillBuffer():
int bytesConsumed = StreamVByte::decodeBulk(
    &ioBatch_[ioBatchPos_],
    128,  // Decode entire buffer at once
    docDeltaBuffer_
);
```

**Advantage**: Better amortization of setup overhead
**Disadvantage**: Requires ensuring 128 docs available (edge case handling)

## Decision

**Status**: ‚úÖ **Implemented, ‚ùì Unmeasured Benefit**

**Keeping the change because**:
1. No performance regression
2. Theoretically correct (SIMD should be faster)
3. Low maintenance cost
4. May benefit larger datasets or different workloads

**Moving on because**:
- P0.2 (Custom Tokenizer): +15-20% indexing (clearer benefit)
- P0.3 (Object Pooling): +5-8% indexing (allocation profiling supports this)
- Further optimization has unclear ROI

## Lessons Learned

1. ‚ùå **Profiling showed "postings traversal" but didn't break down decode vs iteration**
2. ‚ùå **Small dataset size (10k docs) may not show SIMD benefits clearly**
3. ‚úÖ **Enabled SIMD without regression - can revisit later with better profiling**
4. ‚úÖ **Know when to move on - not every optimization shows immediate results**

## Next Steps

1. ‚úÖ Keep SIMD StreamVByte implementation
2. üìù Document as "enabled but unmeasured"
3. ‚û°Ô∏è **Move to P0.2: Custom Tokenizer** (clear 24.65% bottleneck)
4. üîÆ Future: Revisit with better decode-specific profiling

---

**Updated**: 2026-01-30
**Author**: Claude Sonnet 4.5
